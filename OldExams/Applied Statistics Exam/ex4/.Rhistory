dim(doughnuts)
attach(doughnuts)
aov.dough <- aov(prezzo ~ citta + tipo + citta:tipo)
summary(aov.dough)
plot(doughnuts)
# b) Reduce the model:
aov.add <- aov(prezzo ~ citta + tipo)
summary(aov.add)
aov.final <- aov(prezzo ~ tipo)
summary(aov.final)
# c)
# assumption: gaussianity:
levels(tipo)
shapiro.test(prezzo[which(tipo == "crema")])
shapiro.test(prezzo[which(tipo == "liscia")])
shapiro.test(prezzo[which(tipo == "marmellata")])
#OK!
k <- 6
alpha <- 0.05
fit.c <- aov(prezzo ~ citta + tipo + citta:tipo)
summary(fit.c)
p.val <- c(
shapiro.test(prezzo[which(citta==levels(citta)[1] & tipo==levels(tipo)[1])])$p,
shapiro.test(prezzo[which(citta==levels(citta)[1] & tipo==levels(tipo)[2])])$p,
shapiro.test(prezzo[which(citta==levels(citta)[1] & tipo==levels(tipo)[3])])$p,
shapiro.test(prezzo[which(citta==levels(citta)[2] & tipo==levels(tipo)[1])])$p,
shapiro.test(prezzo[which(citta==levels(citta)[2] & tipo==levels(tipo)[2])])$p,
shapiro.test(prezzo[which(citta==levels(citta)[2] & tipo==levels(tipo)[3])])$p)
p.val
bartlett.test(prezzo, citta:tipo)
# Model without interaction (additive model):
# X.ijk = mu + tau.i + beta.j + eps.ijk;
#     eps.ijk~N(0,sigma^2),
#     i=1,2 (effect city), j=1,2,3 (effect type)
fit.c2 <- aov(prezzo ~ citta + tipo)
summary(fit.c2)
# one-way ANOVA
# X.jk = mu + beta.j + eps.ijk;
#     eps.ijk~N(0,sigma^2),
#     j=1,2,3 (effect type)
fit.c3 <- aov(prezzo ~ tipo)
summary(fit.c3)
N <- dim(doughnuts)[1]
g <- length(levels(tipo))
DF <- N-g
alpha <- .05
k <- g+1
qT <- qt(1-alpha/(2*k), DF)
qCinf <- qchisq(1 - alpha / (2*k), DF)
qCsup <- qchisq(alpha / (2*k), DF)
Spooled <- (t(fit.c3$res) %*% fit.c3$res)/DF
Spooled
m1 <- mean(doughnuts[which(tipo==levels(tipo)[1]),1])
m2 <- mean(doughnuts[which(tipo==levels(tipo)[2]),1])
m3 <- mean(doughnuts[which(tipo==levels(tipo)[3]),1])
medie <- c(m1,m2,m3)
ng <- c(length(which(tipo==levels(tipo)[1])),
length(which(tipo==levels(tipo)[2])),
length(which(tipo==levels(tipo)[3])))
BF    <- rbind(cbind(inf=medie - sqrt(c(Spooled) / ng) * qT,
sup=medie + sqrt(c(Spooled) / ng) * qT),
c(inf=Spooled * DF / qCinf,
sup=Spooled * DF / qCsup))
BF
BF    <- rbind(c(inf=medie - sqrt(c(Spooled) / ng) * qT,
sup=medie + sqrt(c(Spooled) / ng) * qT),
c(inf=Spooled * DF / qCinf,
sup=Spooled * DF / qCsup))
BF
BF    <- rbind(cbind(inf=medie - sqrt(c(Spooled) / ng) * qT,
sup=medie + sqrt(c(Spooled) / ng) * qT),
cbind(inf=Spooled * DF / qCinf,
sup=Spooled * DF / qCsup))
BF    <- rbind(cbind(inf=medie - sqrt(c(Spooled) / ng) * qT,
sup=medie + sqrt(c(Spooled) / ng) * qT),
cbind(inf=Spooled * DF / qCinf,
sup=Spooled * DF / qCsup))
BF
BF    <- rbind(cbind(inf=medie - sqrt(c(Spooled) / ng) * qT,
sup=medie + sqrt(c(Spooled) / ng) * qT),
cbind(inf=Spooled * DF / qCinf,
sup=Spooled * DF / qCsup))
BF
detach(doughnuts)
install.packages("fdakma")
setwd("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/Labs/Lab XX - FDA/Lab sessions")
# Using the R package fdakma
library(fdakma)
help(kma)
help(kma.data)
data(kma.data)
names(kma.data)
x <- kma.data$x   # abscissas
y0 <- kma.data$y0 # evaluations of original functions
y1 <- kma.data$y1 # evaluations of original functions' first derivatives
# Plot of original functions
matplot(t(x),t(y0), type='l', xlab='x', ylab='orig.func')
title ('Original functions')
# Plot of original function first derivatives
matplot(t(x),t(y1), type='l', xlab='x', ylab='orig.deriv')
title ('Original function first derivatives')
set.seed(4)
fdakma_example_noalign_0der <- kma(
x=x, y0=y0, n.clust = 3,
warping.method = 'NOalignment',
similarity.method = 'd0.pearson',   # similarity computed as the cosine
# between the original curves
# (correlation)
center.method = 'k-means'
#,seeds = c(1,11,21) # you can give a little help to the algorithm...
)
kma.show.results(fdakma_example_noalign_0der)
fdakma_example_noalign_0der$labels
set.seed(5)
fdakma_example_noalign_1der <- kma(
x=x, y0=y0, y1=y1, n.clust = 3,
warping.method = 'NOalignment',
similarity.method = 'd1.pearson',   # similarity computed as the cosine
# between the first derivatives
# (correlation)
center.method = 'k-means'
#,seeds = c(1,11,21) # you can give a little help to the algorithm...
)
kma.show.results(fdakma_example_noalign_1der)
fdakma_example_noalign_1der$labels
table(fdakma_example_noalign_0der$labels,
fdakma_example_noalign_1der$labels, dnn=c("0der", "1der"))
set.seed(1)
fdakma_example <- kma(
x=x, y0=y0, y1=y1, n.clust = 2,
warping.method = 'affine',
similarity.method = 'd1.pearson',  # similarity computed as the cosine
# between the first derivatives
# (correlation)
center.method = 'k-means'
#seeds = c(1,21) # you can give a little help to the algorithm...
)
kma.show.results(fdakma_example)
graphics.off()
plot(vowels , col=pag+1, asp=1, pch=16, lwd=2)
setwd("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/Labs/Lab 10 - Clustering (Hierarchical, K means) 05052020")
vowels <- read.table('veritatis.txt', header=T)
head(vowels)
dim(vowels)
### question a)
x11()
plot(vowels)
plot(HC, hang=-0.1, sub='', labels=F, xlab='')
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
# data import:
pollution <- read.table("pollution.txt", header = T)
head(pollution)
plot(pollution)
# a) Testing
alpha <- 0.05
# assumption: Gaussianity:
mcshapiro.test(pollution)
n <- dim(pollution)[1]  # 11
p <- dim(pollution)[2]  #  2
x.mean   <- sapply(pollution,mean)
x.cov    <- cov(poll)
x.invcov <- solve(x.cov)
x.cov    <- cov(pollution)
x.invcov <- solve(x.cov)
D.T2 <- n * (x.mean-mu.0) %*% x.invcov %*% (x.mean-mu.0)
mu.0 <- c(50,50)
D.T2 <- n * (x.mean-mu.0) %*% x.invcov %*% (x.mean-mu.0)
x.T2 <- n * (x.mean-mu.0) %*% x.invcov %*% (x.mean-mu.0)
x.T2
cfr.fisher <- ((n-1)*p/(n-p))*qf(1-alpha,p,n-p)
cfr.fisher
D.T2 < cfr.fisher # FALSE: we reject H0 at level 5%
# we compute the p-value
P <- 1-pf(D.T2*(n-p)/(p*(n-1)), p, n-p)
P
# b) ellipse:
plot(pollution, asp=1, pch=1)
ellipse(center=x.mean, shape=x.cov/n, radius=sqrt(cfr.fisher), lwd=2)
# directions:
eigen(x.cov/n)$vectors
# lengths:
sqrt(eigen(x.cov/n)$values*cfr.fisher)
points(mu.0[1], mu.0[2], pch=16, col='grey35', cex=1.5)
abline(h=mu.0[1], v=mu.0[2], col='grey35')
# d)
IC.T2.PM2.5 <- c( x.mean[1]-sqrt(cfr.fisher*x.cov[1,1]/n) , x.mean[1], x.mean[1]+sqrt(cfr.fisher*x.cov[1,1]/n) )
IC.T2.PM10  <- c( x.mean[2]-sqrt(cfr.fisher*x.cov[2,2]/n) , x.mean[2], x.mean[2]+sqrt(cfr.fisher*x.cov[2,2]/n) )
T2 <- rbind(IC.T2.PM2.5, IC.T2.PM10)
dimnames(T2)[[2]] <- c('inf','center','sup')
T2
x.mean
# data import:
stoneflakes <- read.table("stoneflakes.txt", header = T)
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
# data import:
stoneflakes <- read.table("stoneflakes.txt", header = T)
head(stoneflakes)
plot(stoneflakes)
# a) Clustering, Ward:
d <- dist(stoneflakes, method='euclidean')
ward.stones <- hclust(d, method='ward.D2')
plot(ward.stones, main='euclidean-ward', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
# b)
cluster <- cutree(ward.stones, k = 3)
cluster
levels(cluster)
cluster <- as.factor(cluster)
levels(cluster)
# assumptions:
# 1) Normality of each group:
mcshapiro.test(stoneflakes[which(cluster == "1")])
# assumptions:
# 1) Normality of each group:
mcshapiro.test(stoneflakes[which(cluster == "1"),])
mcshapiro.test(stoneflakes[which(cluster == "2"),])
mcshapiro.test(stoneflakes[which(cluster == "3"),])
# OK!!
# 2) Same covariances:
S1 <-  cov(stoneflakes[which(cluster == "1"),])
S2 <-  cov(stoneflakes[which(cluster == "2"),])
S3 <-  cov(stoneflakes[which(cluster == "3"),])
par(mfrow=c(1,3))
image(S1, col=heat.colors(100),main='Cov. S1', asp=1, axes = FALSE, breaks = quantile(rbind(S1,S2,S3), (0:100)/100, na.rm=TRUE))
image(S2, col=heat.colors(100),main='Cov. S2', asp=1, axes = FALSE, breaks = quantile(rbind(S1,S2,S3), (0:100)/100, na.rm=TRUE))
image(S3, col=heat.colors(100),main='Cov. S3', asp=1, axes = FALSE, breaks = quantile(rbind(S1,S2,S3), (0:100)/100, na.rm=TRUE))
# test:
fm <- manova(stoneflakes ~ cluster )
# test:
fm <- manova(as.matrix(stoneflakes) ~ cluster )
summary(fm, test = "Wilks")
table(levels)
table(cluster)
table(cluster)[1]
table(cluster)[1]==32
# c)
i1 <- which(cluster == "1"),]
# c)
i1 <- which(cluster == "1")
i2 <- which(cluster == "2")
i3 <- which(cluster == "3")
W <- summary.manova(fm)$SS$Residuals
n1 <- length(i1)
n2 <- length(i2)
n3 <- length(i3)
n  <- n1+n2+n3
g  <- 3
g  <- 3
p  <- 2
alpha <- 0.05
k <- p*g*(g-1)/2
qT <- qt(1-alpha/(2*k), n-g)
W <- summary.manova(fm)$SS$Residuals
W <- summary.manova(fm)$SS$Residuals
m  <- sapply(stoneflakes,mean)         # estimates mu
m1 <- sapply(stoneflakes[i1,],mean)    # estimates mu.1=mu+tau.1
m2 <- sapply(stoneflakes[i2,],mean)    # estimates mu.2=mu+tau.2
m3 <- sapply(stoneflakes[i3,],mean)    # estimates mu.3=mu+tau.3
inf12 <- m1-m2 - qT * sqrt( diag(W)/(n-g) * (1/n1+1/n2) )
sup12 <- m1-m2 + qT * sqrt( diag(W)/(n-g) * (1/n1+1/n2) )
inf13 <- m1-m3 - qT * sqrt( diag(W)/(n-g) * (1/n1+1/n3) )
sup13 <- m1-m3 + qT * sqrt( diag(W)/(n-g) * (1/n1+1/n3) )
inf23 <- m2-m3 - qT * sqrt( diag(W)/(n-g) * (1/n2+1/n3) )
CI <- list(diff1_2=cbind(inf12, sup12), diff1_3=cbind(inf13, sup13), diff2_3=cbind(inf23, sup23))
CI
inf12 <- m1-m2 - qT * sqrt( diag(W)/(n-g) * (1/n1+1/n2) )
sup12 <- m1-m2 + qT * sqrt( diag(W)/(n-g) * (1/n1+1/n2) )
inf13 <- m1-m3 - qT * sqrt( diag(W)/(n-g) * (1/n1+1/n3) )
sup13 <- m1-m3 + qT * sqrt( diag(W)/(n-g) * (1/n1+1/n3) )
inf23 <- m2-m3 - qT * sqrt( diag(W)/(n-g) * (1/n2+1/n3) )
sup23 <- m2-m3 + qT * sqrt( diag(W)/(n-g) * (1/n2+1/n3) )
CI <- list(diff1_2=cbind(inf12, sup12), diff1_3=cbind(inf13, sup13), diff2_3=cbind(inf23, sup23))
CI
n1 <- length(i1)
n2 <- length(i2)
n3 <- length(i3)
n  <- n1+n2+n3
g  <- 3
p  <- 2
alpha <- 0.1
k <- p*g*(g-1)/2
qT <- qt(1-alpha/(2*k), n-g)
W <- summary.manova(fm)$SS$Residuals
m  <- sapply(stoneflakes,mean)         # estimates mu
m1 <- sapply(stoneflakes[i1,],mean)    # estimates mu.1=mu+tau.1
m2 <- sapply(stoneflakes[i2,],mean)    # estimates mu.2=mu+tau.2
m3 <- sapply(stoneflakes[i3,],mean)    # estimates mu.3=mu+tau.3
inf12 <- m1-m2 - qT * sqrt( diag(W)/(n-g) * (1/n1+1/n2) )
sup12 <- m1-m2 + qT * sqrt( diag(W)/(n-g) * (1/n1+1/n2) )
inf13 <- m1-m3 - qT * sqrt( diag(W)/(n-g) * (1/n1+1/n3) )
sup13 <- m1-m3 + qT * sqrt( diag(W)/(n-g) * (1/n1+1/n3) )
inf23 <- m2-m3 - qT * sqrt( diag(W)/(n-g) * (1/n2+1/n3) )
sup23 <- m2-m3 + qT * sqrt( diag(W)/(n-g) * (1/n2+1/n3) )
CI <- list(diff1_2=cbind(inf12, sup12), diff1_3=cbind(inf13, sup13), diff2_3=cbind(inf23, sup23))
CI
plot(ward.stones, main='euclidean-ward', hang=-0.1, xlab='',,ylab='distance' labels=F, cex=0.6, sub='')
plot(ward.stones, main='euclidean-ward', hang=-0.1, xlab='',,ylab='distance', labels=F, cex=0.6, sub='')
#
airfoil <- read.table("airfoil.txt", header = T)
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
#
airfoil <- read.table("airfoil.txt", header = T)
head(airfoil)
attach(airfoil)
detach(airfoil)
attach(airfoil)
detach(airfoil)
attach(airfoil)
plot(frequency, sound)
fm <- lm(sound ~ velocity + frequency + frequency:velocity)
summary(fm)
# diagnostics of residuals
par(mfrow=c(2,2))
plot(fm)
# sound vs. velocity:
C <- rbind(c(0,1,0,0), c(0,0,0,1))
linearHypothesis(fm, C, c(0,0))
# sound vs. frequency:
C <- rbind(c(0,0,1,0), c(0,0,0,1))
linearHypothesis(fm, C, c(0,0))
# c)
rm <- lm(sound ~ velocity + frequency)
summary(rm)
head(airfoil)
# prediction:
z0 <- data.frame(frequency= 15000, velocity = 'H')
predict(rm, z0, interval = "confidence")
predict(rm, z0, interval = "confidence", level = 0.01)
fm <- lm(sound ~ velocity + frequency + frequency:velocity)
summary(fm)
# parameters estimate
coeff(fm)
# parameters estimate
coef(fm)
coef(1)+coef(2)
coef[1]+coef[2]
coeffs[1]+coeffs[2]
# parameters estimate
coeffs = coef(fm)
coeffs[1]+coeffs[2]
coeffs[3]+ coeffs[4]
shapiro.test(fm$residuals)
linearHypothesis(fm, C, c(0,0))
# sound vs. velocity:
C <- rbind(c(0,1,0,0), c(0,0,0,1))
linearHypothesis(fm, C, c(0,0))
summary(rm)
c_rm <- coef(rm)
c_rm(1)-c_rm(2)
c_rm[1 ]-c_rm[2]
c_rm[1 ]+c_rm[2]
predict(rm, z0, interval = "confidence", level = 0.01)
detach(airfoil)
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
library(sp)           ## Data management
library(lattice)      ## Data management
library(geoR)         ## Geostatistics
library(gstat)        ## Geostatistics
# data import:
revenues <- read.table("revenues.txt", header = T)
head(revenues)
coordinates(revenues) <- c('x','y')
attach(revenues)
coordinates(revenues) <- c('x','y')
# a)
var.est <- variogram(revenue ~ population, revenues)
plot(var.est)
# fitted variogram
v.fit <- fit.variogram(var.est, vgm(psill = 600, model = 'Gau', range = 1000))
v.fit
plot(var.est,v.fit)
g.model <- gstat(formula = revenue ~ population, data = revenues, model = v.fit)
# estimation of a0, a1:
s0 <- data.frame(x = c(514711,514711), y = c(5033903,5033903), population=c(0,1))
coordinates(s0) <- c('x', 'y')
predict(g.1, s0, BLUE = F)
predict(g.model, s0, BLUE = F)
prediction <- predict(g.model, s0, BLUE = F)
# a0: first one
prediction$var1.pred[1]
# a1: second minus first
prediction$var1.pred[2]- prediction$var1.pred[1]
# b) Linear model:
fm = lm(population ~ distance)
# data import:
revenues <- read.table("revenues.txt", header = T)
head(revenues)
attach(revenues)
coordinates(revenues) <- c('x','y')
# a)
# estimated variogram
var.est <- variogram(revenue ~ population, revenues)
plot(var.est)
# fitted variogram
v.fit <- fit.variogram(var.est, vgm(psill = 600, model = 'Gau', range = 1000))
v.fit
plot(var.est,v.fit)
g.model <- gstat(formula = revenue ~ population, data = revenues, model = v.fit)
# estimation of a0, a1:
s0 <- data.frame(x = c(514711,514711), y = c(5033903,5033903), population=c(0,1))
coordinates(s0) <- c('x', 'y')
prediction <- predict(g.model, s0, BLUE = F)
# a0: first one
prediction$var1.pred[1]
# a1: second minus first
prediction$var1.pred[2]- prediction$var1.pred[1]
# b) Linear model:
fm = lm(population ~ distance)
summary(fm)
s0 <- data.frame(x =514703.8, y = 5035569.3)
sD <- data.frame(x = 514711.6, y = 5033903)
sdiff <- sB - sD
sB <- data.frame(x =514703.8, y = 5035569.3)
sD <- data.frame(x = 514711.6, y = 5033903)
sdiff <- sB - sD
sdiff
dB <- dist(sdiff)
pB <- predict(fm, dB)
dB <- dist(sdiff)
dB
dB <- dist(c(sB,sD))
dB
distB <- sqrt(sdiff$x^2+sdiff$y^2)
pB <- predict(fm, distB)
pB <- predict(fm, distance=distB)
pB
summary(fm)
d0 <- data.frame(distance = distB)
pB <- predict(fm, d0)
pB
p0 <- data.frame(ppopulation = pB)
predict(g.model,p0, BLUE = F)
p0 <- data.frame(population = pB)
predict(g.model,p0, BLUE = F)
p0 <- data.frame(x = 514703.8,y = 5035569.3 ,population = pB)
coordinates(p0) <- c('x','y')
predict(g.model,p0, BLUE = F)
pB
p0 <- data.frame(x = 514703.8,y = 5035569.3 ,population = pB)
revenues
prediction <- predict(g.model, s0, BLUE =T)
# a0: first one
prediction$var1.pred[1]
# a1: second minus first
prediction$var1.pred[2]- prediction$var1.pred[1]
# estimation of a0, a1:
s0 <- data.frame(x = c(514711,514711), y = c(5033903,5033903), population=c(0,1))
coordinates(s0) <- c('x', 'y')
prediction <- predict(g.model, s0, BLUE =T)
# a0: first one
prediction$var1.pred[1]
# a1: second minus first
prediction$var1.pred[2]- prediction$var1.pred[1]
# b) Linear model:
fm = lm(population ~ distance)
summary(fm)
sB <- data.frame(x =514703.8, y = 5035569.3)
sD <- data.frame(x = 514711.6, y = 5033903)
sdiff <- sB - sD
sdiff
distB <- sqrt(sdiff$x^2+sdiff$y^2)
dB <- dist(c(sB,sD))
d0 <- data.frame(distance = distB)
pB <- predict(fm, d0)
pB
p0 <- data.frame(x = 514703.8,y = 5035569.3 ,population = pB)
coordinates(p0) <- c('x','y')
predict(g.model,p0, BLUE = F)
head(revenues)
