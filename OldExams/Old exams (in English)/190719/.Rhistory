image.plot(time,time,(cov(t(eval.1))[1:365,]))
eval.2 <- eval.fd(time,data_W.fd.2)
image.plot(time,time,(cor(t(eval.2))[1:365,]))
eval.3 <- eval.fd(time,data_W.fd.3)
image.plot(time,time,(cov(t(eval.3))[1:365,]))
graphics.off()
# Second dataset: lip
# 51 measurements of the position of the lower lip every 7
# milliseconds for 20 repetitions of the syllable 'bob'.
help(lip)
data_L <- lip
time <- seq(0,350,by=7)
matplot(time,data_L,type='l',main='Lip data',ylab='Position',
xlab='Time (millisec.)')
basis <- create.fourier.basis(rangeval=c(0,350),nbasis=51)
data_L.fd <- Data2fd(data_L,time,basis)
plot.fd(data_L.fd, main="Fourier")
# Better to use a b-spline basis
basis <- create.bspline.basis(rangeval=c(0,350),nbasis=21)
data_L.fd <- Data2fd(y = data_L,argvals = time,basisobj = basis)
plot.fd(data_L.fd, main="B-splines")
# Estimate of the mean and of the covariance kernel
layout(cbind(1,2))
plot.fd(data_L.fd,xaxs='i')
lines(mean(data_L.fd),lwd=2)
eval <- eval.fd(time,data_L.fd)
image.plot(time, time, (cov(t(eval))[1:51,]))
graphics.off()
##### FPCA #####
help(pca.fd)
# interpolated data (Choice 1)
plot.fd(data_W.fd.1,ylab='temperature')
pca_W.1 <- pca.fd(data_W.fd.1,nharm=5,centerfns=TRUE)
# scree plot
# pca.fd computes all the 365 eigenvalues, but only the first
# N-1=34 are non-null
plot(pca_W.1$values[1:35],xlab='j',ylab='Eigenvalues')
plot(cumsum(pca_W.1$values)[1:35]/sum(pca_W.1$values),xlab='j',ylab='CPV',ylim=c(0.8,1))
# first two FPCs
layout(cbind(1,2))
plot(pca_W.1$harmonics[1,],col=1,ylab='FPC1',ylim=c(-0.1,0.08))
abline(h=0,lty=2)
plot(pca_W.1$harmonics[2,],col=2,ylab='FPC2',ylim=c(-0.1,0.08))
# plot of the FPCs as perturbation of the mean
media <- mean(data_W.fd.1)
plot(media,lwd=2,ylim=c(-25,20),ylab='temperature',main='FPC1')
lines(media+pca_W.1$harmonics[1,]*sqrt(pca_W.1$values[1]), col=2)
lines(media-pca_W.1$harmonics[1,]*sqrt(pca_W.1$values[1]), col=3)
plot(media,lwd=2,ylim=c(-20,20),ylab='temperature',main='FPC2')
lines(media+pca_W.1$harmonics[2,]*sqrt(pca_W.1$values[2]), col=2)
lines(media-pca_W.1$harmonics[2,]*sqrt(pca_W.1$values[2]), col=3)
# Command of the library fda that automatically does these plots
par(mfrow=c(1,2))
plot.pca.fd(pca_W.1, nx=100, pointplot=TRUE, harm=c(1,2), expand=0, cycle=FALSE)
graphics.off()
###
# smooth data (Choice 2)
plot.fd(data_W.fd.2)
pca_W.2 <- pca.fd(data_W.fd.2,nharm=5,centerfns=TRUE)
# scree plot
plot(pca_W.2$values,xlab='j',ylab='Eigenvalues')
plot(cumsum(pca_W.2$values)/sum(pca_W.2$values),xlab='j',ylab='CPV',ylim=c(0.8,1))
# first two FPCs
layout(cbind(1,2))
plot(pca_W.2$harmonics[1,],col=1,ylab='FPC1',ylim=c(-0.1,0.08))
abline(h=0,lty=2)
plot(pca_W.2$harmonics[2,],col=2,ylab='FPC2',ylim=c(-0.1,0.08))
# plot of the FPCs as perturbation of the mean
media <- mean(data_W.fd.2)
plot(media,lwd=2,ylim=c(-25,20),ylab='temperature',main='PC1')
lines(media+pca_W.2$harmonics[1,]*sqrt(pca_W.2$values[1]), col=2)
lines(media-pca_W.2$harmonics[1,]*sqrt(pca_W.2$values[1]), col=3)
plot(media,lwd=2,ylim=c(-20,20),ylab='temperature',main='PC2')
lines(media+pca_W.2$harmonics[2,]*sqrt(pca_W.2$values[2]), col=2)
lines(media-pca_W.2$harmonics[2,]*sqrt(pca_W.2$values[2]), col=3)
graphics.off()
# scatter plot of the scores
par(mfrow=c(1,2))
plot(pca_W.1$scores[,1],pca_W.1$scores[,2],xlab="Scores FPC1",ylab="Scores FPC2",lwd=2)
points(pca_W.1$scores[35,1],pca_W.1$scores[35,2],col=2, lwd=4)
plot(pca_W.1$scores[,1],pca_W.1$scores[,2],type="n",xlab="Scores FPC1",
ylab="Scores FPC2",xlim=c(-400,250))
text(pca_W.1$scores[,1],pca_W.1$scores[,2],dimnames(data_W)[[2]], cex=1)
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
library(fda)
# data import:
data <- read.table("watertemp.txt", header = T)
head(data)
matplot(t(data[,1:365]),type = "l")
basis <- create.fourier.basis(rangeval = c(1,365),nbasis=45)
basis1 <- create.bspline.basis(rangeval = c(1,365),nbasis=10)
plot(basis)
plot(basis1)
Xsp <- smooth.basis(argvals = 1:365, y = t(data[,1:365]), fdParobj = basis)
# coefficients here:
head(Xsp)
head(Xsp$fd$coefs)
smoothed <- eval.fd(1:365, Xsp$fd)
head(smoothed)
matplot(smoothed, type="l")
pca.wt <- pca.fd(Xsp$fd, nharm = 5, centerfns = T)
# variance explained:
sum(pca.wt$varprop)
plot(pca.wt)
plot(pca.wt)
# other possibility:
mean.F <- mean(Xsp$fd)
mean.F <- eval.fd(1:365, mean.F)
x11()
par(mfrow = c(1,3))
plot(mean.F,type = 'l' )
harm1 <- mean.F + sd(pca.wt$scores[,1])*eval.fd(1:365,pca.wt$harmonics[1,])
harm2 <- mean.F - sd(pca.wt$scores[,1])*eval.fd(1:365,pca.wt$harmonics[1,])
lines(1:365, harm1, col="palegreen1")
lines(1:365, harm2, col="indianred1")
plot(mean.F,type = 'l' )
harm1 <- mean.F + sd(pca.wt$scores[,2])*eval.fd(1:365,pca.wt$harmonics[2,])
harm2 <- mean.F - sd(pca.wt$scores[,2])*eval.fd(1:365,pca.wt$harmonics[2,])
lines(1:365, harm1, col="palegreen1")
lines(1:365, harm2, col="indianred1")
plot(mean.F,type = 'l' )
harm1 <- mean.F + sd(pca.wt$scores[,3])*eval.fd(1:365,pca.wt$harmonics[3,])
harm2 <- mean.F - sd(pca.wt$scores[,3])*eval.fd(1:365,pca.wt$harmonics[3,])
lines(1:365, harm1, col="palegreen1")
lines(1:365, harm2, col="indianred1")
layout(cbind(1,2))
plot(pca.wt$harmonics[1,], col = 1, ylab = 'FPC1')
abline(h=0, lty=2)
plot(pca.wt$harmonics[2,], col = 2, ylab = 'FPC2')
# first two components:
layout(cbind(1,2))
plot(pca.wt$harmonics[1,], col = 1, ylab = 'FPC1')
plot(pca.wt$harmonics[2,], col = 2, ylab = 'FPC2')
# scree plot:
par(mfrow=c(1,1))
plot(cumsum(pca.wt$varprop)/sum(pca.wt$varprop), type='b', ylim = c(0,1))
# c) Scores along first 2 PCs:
# which is which:
i1 <- which(data$Zone == "Surface")
i2 <- which(data$Zone == "Medium")
i3 <- which(data$Zone == "Deep")
col.stat <- rep(NA, 132)
col.stat[i1] <- "lightblue"
col.stat[i2] <- "steelblue1"
col.stat[i3] <- "navy"
mat.scores1 <- matrix(0,365,132)
for (i in 1:132){
mat.scores1[,i]=pca.wt$scores[i,1]*eval.fd(1:365, pca.wt$harmonics[1,])
}
mat.scores1 <- matrix(0,365,132)
for (i in 1:132){
mat.scores1[,i]=pca.wt$scores[i,1]*eval.fd(1:365, pca.wt$harmonics[1,])
}
matplot(1:365, mat.scores1, col = col.stat, type = "l")
mat.scores2 <- matrix(0,365,132)
for (i in 1:132){
mat.scores2[,i]=pca.wt$scores[i,2]*eval.fd(1:365, pca.wt$harmonics[2,])
}
matplot(1:365, mat.scores2, col = col.stat, type = "l")
mat.scores3 <- matrix(0,365,132)
for (i in 1:132){
mat.scores3[,i]=mean.F + pca.wt$scores[i,1]*eval.fd(1:365, pca.wt$harmonics[1,])pca.wt$scores[i,2]*eval.fd(1:365, pca.wt$harmonics[2,])
}
mat.scores3 <- matrix(0,365,132)
for (i in 1:132){
mat.scores3[,i]=mean.F + pca.wt$scores[i,1]*eval.fd(1:365, pca.wt$harmonics[1,])+pca.wt$scores[i,2]*eval.fd(1:365, pca.wt$harmonics[2,])
}
matplot(1:365, mat.scores2, col = col.stat, type = "l")
matplot(1:365, mat.scores3, col = col.stat, type = "l")
# we can also plot the PCs classically:
plot(pca_wt.1$scores[,1],pca_W.1$scores[,2],col = col.stat,xlab="Scores FPC1",ylab="Scores FPC2",lwd=2)
# we can also plot the PCs classically:
plot(pca.wt$scores[,1],pca.wt$scores[,2],col = col.stat,xlab="Scores FPC1",ylab="Scores FPC2",lwd=2)
# data reading:
piadeina <- read.table("piadeina.txt", header=T)
head(piadeina)
plot(piadeina)
attach(piadeina)
levels(day)
levels(Day.of.Week)
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
# data reading:
piadeina <- read.table("piadeina.txt", header=T)
head(piadeina)
plot(piadeina)
attach(piadeina)
levels(Day.of.Week)
# - building the categorical variables day of week:
dF <- rep(0,)
# a) linear regression model:
n <- dim(piadeina)[1]
# - building the categorical variables day of week:
dTue <- dWed <- dThu <- dFri <- drep(0,n)
# - building the categorical variables day of week:
dTue <- dWed <- dThu <- dFri <- rep(0,n)
dTue[which(Day.of.Week == "Tue")] <- 1
dWed[which(Day.of.Week == "Wed")] <- 1
dThu[which(Day.of.Week == "Thu")] <- 1
dFri[which(Day.of.Week == "Fri")] <- 1
fm <- lm (Sales ~ dTue + dWed + dThu + dFri + Bread.Sold +
Wraps.Sold + Sandwich.Sold + Focaccia.Sold +
Piadina.Sold + Chips.Sold + Juices.Sold +
Total.Soda.and.Coffee.Sold + Max.Daily.Temperature)
summary(fm)
plot(residuals(fm))
# assumptions:
par(mfrow= c(2,2))
plot(residuals(fm))
plot(fm)
# assumptions:
par(mfrow= c(2,2))
plot(fm)
shapiro.test(residuals(fm))
View(piadeina)
View(piadeina)
# b) Lasso method:
x <- model.matrix(Sales ~ dTue + dWed + dThu + dFri + Bread.Sold +
Wraps.Sold + Sandwich.Sold + Focaccia.Sold +
Piadina.Sold + Chips.Sold + Juices.Sold +
Total.Soda.and.Coffee.Sold + Max.Daily.Temperature)[,-1]
x
# Build the vector of response
y <- Sales
y
fit.lasso <- glmnet(x,y, lambda = 5) # default: alpha=1 -> lasso
library(glmnet)
fit.lasso <- glmnet(x,y, lambda = 5) # default: alpha=1 -> lasso
# [note: if alpha=0 -> ridge regression]
summary(fit.lasso)
# [note: if alpha=0 -> ridge regression]
coef(fit.lasso)
# c)
# Let's set a grid of candidate lambda's for the estimate
lambda.grid <- seq(0,100,length=100)
lambda.grid
# c)
# Let's set a grid of candidate lambda's for the estimate
lambda.grid <- seq(0,100,by = 1)
lambda.grid
fit.lasso <- glmnet(x,y, lambda = lambda.grid)
# cross - validation:
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid) # default: 10-fold CV
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')[1:3,]
coef.lasso
cv.lasso
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')[,]
coef.lasso
# data import:
pines <- read.table("pines.txt", header = T)
head(pines)
plot(pines)
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
# data import:
pines <- read.table("pines.txt", header = T)
head(pines)
plot(pines)
# a) Testing:
# assumption: Gaussianity.
mcshapiro.test(pines)
# OKay! p value 10%
alpha <- 0.01
mu0 <- c(14.2350,42.4520)
x.mean <- sapply(pines, mean)
x.mean
x.cov <- cov(pines)
x.invcov <- solve(x.cov)
n <- dim(pines)[1]
p <- dim(pines)[2]
x.T2       <- n * (x.mean-mu0) %*% x.invcov %*% (x.mean-mu0)
# Radius of the ellipsoid
cfr.fisher <- ((n-1)*p/(n-p))*qf(1-alpha,p,n-p)
# Test:
x.T2 < cfr.fisher   # no statistical evidence to reject H0 at level alpha
# Compute the p-value
P <- 1-pf(x.T2*(n-p)/((n-1)*p), p, n-p)
P
help(pf)
# Compute the p-value
P <- 1-pf(x.T2*(n-p)/((n-1)*p), p, n-p)
P
# b) Elliptical region:
plot(pines, asp = 1)
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
help("ellipse")
remove.packages(ellipse)
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
# b) Elliptical region:
plot(pines, asp = 1)
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
remove.packages(ellipse,ellipse)
remove.packages(ellipse)
remove.packages("ellipse", lib="~/R/win-library/3.6")
# b) Elliptical region:
plot(pines, asp = 1)
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
# data import:
pines <- read.table("pines.txt", header = T)
head(pines)
plot(pines)
# a) Testing:
# assumption: Gaussianity.
mcshapiro.test(pines)
# OKay! p value 10%
alpha <- 0.01
mu0 <- c(14.2350,42.4520)
x.mean <- sapply(pines, mean)
x.cov <- cov(pines)
x.invcov <- solve(x.cov)
n <- dim(pines)[1]
p <- dim(pines)[2]
x.T2       <- n * (x.mean-mu0) %*% x.invcov %*% (x.mean-mu0)
# Radius of the ellipsoid
cfr.fisher <- ((n-1)*p/(n-p))*qf(1-alpha,p,n-p)
# Test:
x.T2 < cfr.fisher   # no statistical evidence to reject H0 at level alpha
# Compute the p-value
P <- 1-pf(x.T2*(n-p)/((n-1)*p), p, n-p)
P
# b) Elliptical region:
plot(pines, asp = 1)
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
help("ellipse")
# set wd
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# clean workspace:
graphics.off()
rm(list=ls())
# load the function for m shapiro test:
load("C:/Users/Marco Paggiaro/OneDrive/Desktop/mieidati/Applied Statistics/mcshapiro.test.RData")
# Import all the libraries, just because
library(class)
library(MASS)
library(car)
# data import:
pines <- read.table("pines.txt", header = T)
head(pines)
plot(pines)
# a) Testing:
# assumption: Gaussianity.
mcshapiro.test(pines)
# OKay! p value 10%
alpha <- 0.01
mu0 <- c(14.2350,42.4520)
x.mean <- sapply(pines, mean)
x.cov <- cov(pines)
x.invcov <- solve(x.cov)
n <- dim(pines)[1]
p <- dim(pines)[2]
x.T2       <- n * (x.mean-mu0) %*% x.invcov %*% (x.mean-mu0)
# Radius of the ellipsoid
cfr.fisher <- ((n-1)*p/(n-p))*qf(1-alpha,p,n-p)
# Test:
x.T2 < cfr.fisher   # no statistical evidence to reject H0 at level alpha
# Compute the p-value
P <- 1-pf(x.T2*(n-p)/((n-1)*p), p, n-p)
P
# b) Elliptical region:
plot(pines, asp = 1)
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
car$ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
{car}ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
library(car)
library(mvtnorm)
setwd("D:/RTDA/Didattica/Applied Statistics MATE 19-20/Lab 5 - 16042020")
load("D:/RTDA/Didattica/Applied Statistics MATE 19-20/Lab 5 - 16042020/mcshapiro.test.RData")
box_cox <- function(x,lambda)
{
if(lambda!=0)
return((x^lambda-1)/lambda)
return(log(x))
}
x<-seq(0,25,by=0.01)
x11()
plot(x,box_cox(x,0),col='gold',type='l',xlab='x',ylab=expression(x[lambda]),ylim=c(-20,30),xlim=c(-5,25),asp=1)
title(main='Traformazioni di Box-Cox')
curve(box_cox(x,-1),from=0,to=25,col='blue',add=TRUE)
curve(box_cox(x,1),from=0,to=25,col='red',add=TRUE)
curve(box_cox(x,2),from=0,to=25,col='springgreen',add=TRUE)
points(1,0,pch=19,col='black')
abline(v=0,lty=2,col='grey')
legend('topright',c(expression(paste(lambda,'=-1')),expression(paste(lambda,'=0')),
expression(paste(lambda,'=1')),expression(paste(lambda,'=2'))),col=c('blue','gold','red','springgreen'),lty=c(1,1,1,1,1))
xx=seq(0.01,20.01,.05)
par(cex=.5)
points(xx, rep(0,length(xx)), col='grey', pch=19)
points(-.5+rep(0,length(xx)),box_cox(xx,-1),  col='blue', pch=19)
points(-.5+rep(-.5,length(xx)),log(xx),  col='gold', pch=19)
points(-.5+rep(-1,length(xx)),box_cox(xx,1), col='red', pch=19)
points(-.5+rep(-1.5,length(xx)),box_cox(xx,2), col='springgreen', pch=19)
points(1,0,pch=19,col='black')
graphics.off()
##### A trivial example
n <- 100
set.seed(05042016)
x <- rnorm(n)^2
x11()
hist(x, col='grey', prob=T, xlab='x', main ='Histogram of X')
points(x,rep(0,n), pch=19)
# Univariate Box-Cox transformation
# We compute the optimal lambda of the univariate Box-Cox transformation
# (command powerTransform of library car)
lambda.x <- powerTransform(x)
lambda.x
# Transformed sample with the optimal lambda (command bcPower of library car)
bc.x <- bcPower(x, lambda.x$lambda)      # it transforms the data of the first argument through the Box-Cox
# transformation with lambda given as second argument
hist(bc.x, col='grey', prob=T, main='Histogram of BC(X)', xlab='BC(x)')
points(bc.x,rep(0,n), pch=19)
shapiro.test(x)
shapiro.test(bc.x)
### Multivariate Box-Cox transformation (Johnson-Wichern Cap.4.8)
# Similar to univariate transformation, but jointly on all the variables
rm(x)
##### Simulated Example
b <- read.table('data_sim.txt')
head(b)
dim(b)
attach(b)
x11()
plot(b,pch=19,main='Data', xlim=c(1,7), ylim=c(-20,800))
points(x, rep(-20,dim(b)[1]), col='red', pch=19)
points(rep(1,dim(b)[1]), y, col='blue', pch=19)
x11()
graphics.off()
mu <- c(1,0)
sig <- matrix(c(1,1,1,2),nrow=2)
set.seed(123)
x <- rmvnorm(n=30, mean=mu, sigma=sig)
x <- data.frame(X.1=x[,1],X.2=x[,2])
x11()
plot(x, asp = 1,pch=19)
dev.off()
n <- dim(x)[1]
p <- dim(x)[2]
x.mean   <- sapply(x,mean)
x.cov    <- cov(x)
x.invcov <- solve(x.cov)
#_______________________________________________________________________________
### Test on the mean of level alpha=1%
### H0: mu == mu0 vs H1: mu != mu0
### with mu0=c(1,0)
###-----------------------------------
mcshapiro.test(x)
alpha <- 0.01
mu0 <- c(1,0)
# T2 Statistics
x.T2       <- n * (x.mean-mu0) %*% x.invcov %*% (x.mean-mu0)
# Radius of the ellipsoid
cfr.fisher <- ((n-1)*p/(n-p))*qf(1-alpha,p,n-p)
# Test:
x.T2 < cfr.fisher   # no statistical evidence to reject H0 at level alpha
# Compute the p-value
P <- 1-pf(x.T2*(n-p)/((n-1)*p), p, n-p)
P
x11()
xx <- seq(0,40,by=0.05)
plot(xx,df(xx*(n-p)/((n-1)*p),p,n-p),type="l",lwd=2,main='Density F(p,n-p)',xlab='x*(n-p)/((n-1)*p)',ylab='Density')
abline(h=0,v=x.T2*(n-p)/((n-1)*p),col=c('grey','red'),lwd=2,lty=c(2,1))
dev.off()
# Region of rejection (centred in mu0)
x11()
plot(x, asp = 1)
ellipse(mu0, shape=x.cov/n, sqrt(cfr.fisher), col = 'blue', lty = 2, center.pch = 16)
? ellipse
